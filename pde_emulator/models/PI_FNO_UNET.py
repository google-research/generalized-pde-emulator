# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import jax.nn as jnn
import jax.random as jr
import equinox as eqx
import jax.numpy as jnp
from typing import List, Callable

def compute_periodic_derivatives(u: jnp.ndarray) -> jnp.ndarray:
    """
    Computes 1st, 2nd, 3rd, and 4th spatial derivatives of u using central
    finite differences with periodic boundary conditions, AND u^2, AND u*u_x term.
    u shape: (channels, spatial_dims)
    Returns: concatenated derivatives and u^2 (6 * channels, spatial_dims)
    """
    # Assuming dx = 1 for simplicity; the network learns to scale
    
    # Non-linear terms
    u_squared = u**2 # Shape: (channels, spatial_dims)
    
    # 1st order central difference
    u_x = (jnp.roll(u, shift=-1, axis=-1) - jnp.roll(u, shift=1, axis=-1)) / 2.0
    
    # Non-linear advection term (NEW ADDITION)
    u_u_x = u * u_x # Shape: (channels, spatial_dims)

    # 2nd order central difference
    u_xx = (jnp.roll(u, shift=-1, axis=-1) - 2 * u + jnp.roll(u, shift=1, axis=-1))

    # 3rd order central difference
    u_xxx = (jnp.roll(u_xx, shift=-1, axis=-1) - jnp.roll(u_xx, shift=1, axis=-1)) / 2.0

    # 4th order central difference
    u_xxxx = (jnp.roll(u_xx, shift=-1, axis=-1) - 2 * u_xx + jnp.roll(u_xx, shift=1, axis=-1))
    
    # Concatenate u_squared, u_u_x, u_x, u_xx, u_xxx, u_xxxx along the channel dimension
    # If u has shape (1, N), output will be (1+1+4, N) = (6, N) from this function.
    # The original u will be concatenated in the EquationAwareModel.
    return jnp.concatenate([u_squared, u_u_x, u_x, u_xx, u_xxx, u_xxxx], axis=0)


# A small MLP to process the equation encoding vector
class EquationEncoder(eqx.Module):
    layers: List[eqx.nn.Linear]
    activation: Callable

    def __init__(self, in_dim: int, out_dim: int, hidden_dim: int, activation: Callable, *, key):
        key1, key2, key3 = jr.split(key, 3)
        self.layers = [
            eqx.nn.Linear(in_dim, hidden_dim, key=key1),
            eqx.nn.Linear(hidden_dim, hidden_dim, key=key2),
            eqx.nn.Linear(hidden_dim, out_dim, key=key3)
        ]
        self.activation = activation

    def __call__(self, x):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i < len(self.layers) - 1:
                x = self.activation(x)
        return x

# A FiLM (Feature-wise Linear Modulation) Layer
class FiLMLayer(eqx.Module):
    to_gamma_beta: eqx.nn.Linear

    def __init__(self, embedding_dim: int, num_channels: int, *, key):
        self.to_gamma_beta = eqx.nn.Linear(embedding_dim, 2 * num_channels, key=key)

    def __call__(self, x, embedding):
        gamma_beta = self.to_gamma_beta(embedding)
        gamma, beta = jnp.split(gamma_beta, 2)
        gamma = jnp.expand_dims(gamma, axis=-1)
        beta = jnp.expand_dims(beta, axis=-1)
        return x * gamma + beta

# FiLM-conditioned Spectral Convolution Layer (core FNO operation)
class FiLMedSpectralConv1d(eqx.Module):
    """
    Applies a Fourier transform, multiplies by embedding-conditioned complex weights,
    and inverse Fourier transforms. The weights are generated by an MLP from the embedding.
    """
    in_channels: int
    out_channels: int
    modes: int
    spectral_weights_generator: eqx.nn.Linear
    spectral_bias_generator: eqx.nn.Linear # For spectral bias

    def __init__(self, in_channels: int, out_channels: int, embedding_dim: int, modes: int, *, key):
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes = modes
        key_w, key_b = jr.split(key)

        # Generate complex weights (real and imaginary parts for out_c x in_c matrix per mode)
        # Weights shape: (out_channels, in_channels, modes)
        self.spectral_weights_generator = eqx.nn.Linear(embedding_dim, 2 * out_channels * in_channels * modes, key=key_w)
        # Bias shape: (out_channels, modes)
        self.spectral_bias_generator = eqx.nn.Linear(embedding_dim, 2 * out_channels * modes, key=key_b)

    def __call__(self, x: jnp.ndarray, embedding: jnp.ndarray) -> jnp.ndarray:
        # x: (channels, spatial_dims)
        N_spatial = x.shape[-1]

        # 1. Fourier transform input
        x_fft = jnp.fft.rfft(x) # (in_channels, N_spatial/2 + 1)

        # Apply weights in Fourier space for the truncated modes
        # x_fft_truncated: (in_channels, modes)
        x_fft_truncated = x_fft[:, :self.modes]

        # Generate complex weights and biases from embedding
        # Weights: (out_channels, in_channels, modes)
        raw_weights = self.spectral_weights_generator(embedding).reshape(
            self.out_channels, self.in_channels, 2, self.modes
        )
        weights = jnp.complex64(raw_weights[:, :, 0, :] + 1j * raw_weights[:, :, 1, :])

        # Bias: (out_channels, modes)
        raw_bias = self.spectral_bias_generator(embedding).reshape(
            self.out_channels, 2, self.modes
        )
        bias = jnp.complex64(raw_bias[:, 0, :] + 1j * raw_bias[:, 1, :])

        # Batched matrix multiplication for each mode: (out_c, in_c, modes) @ (in_c, modes) -> (out_c, modes)
        out_fft_truncated = jnp.einsum('oim,im->om', weights, x_fft_truncated) + bias

        # Pad with zeros for higher modes (Nyquist frequency related)
        out_fft_full = jnp.zeros((self.out_channels, N_spatial // 2 + 1), dtype=jnp.complex64)
        out_fft_full = out_fft_full.at[:, :self.modes].set(out_fft_truncated)

        # 3. Inverse Fourier transform
        x_pred = jnp.fft.irfft(out_fft_full, n=N_spatial)
        return x_pred

# A FiLMed Fno-based Block
class FiLMedFnoBlock(eqx.Module):
    conv: eqx.nn.Conv1d
    film_conv: FiLMLayer
    fno: FiLMedSpectralConv1d
    film_fno: FiLMLayer
    activation: Callable

    def __init__(self, num_channels: int, embedding_dim: int, modes: int, activation: Callable, *, key):
        key_conv, key_film_conv, key_fno, key_film_fno = jr.split(key, 4)
        self.activation = activation

        # Spatial convolution path
        self.conv = eqx.nn.Conv1d(num_channels, num_channels, kernel_size=3, padding=1, key=key_conv)
        self.film_conv = FiLMLayer(embedding_dim, num_channels, key=key_film_conv)

        # Spectral convolution path
        self.fno = FiLMedSpectralConv1d(num_channels, num_channels, embedding_dim, modes, key=key_fno)
        self.film_fno = FiLMLayer(embedding_dim, num_channels, key=key_film_fno)

    def __call__(self, x, embedding):
        residual = x

        # Path 1: Spatial convolution + FiLM + Activation
        x_conv = self.conv(x)
        x_conv = self.film_conv(x_conv, embedding)
        x_conv = self.activation(x_conv)

        # Path 2: Spectral convolution (FNO) + FiLM + Activation
        x_fno = self.fno(x, embedding)
        x_fno = self.film_fno(x_fno, embedding)
        x_fno = self.activation(x_fno)

        # Combine paths (sum) and add residual
        return x_conv + x_fno + residual

# The main network, now a U-Net structure (NEW ARCHITECTURE)
class FiLMedFnoUNet(eqx.Module):
    in_proj: eqx.nn.Conv1d
    # Encoder Path
    encoder_fno_block: FiLMedFnoBlock
    down_conv: eqx.nn.Conv1d
    # Bottleneck Path
    bottleneck_fno_block: FiLMedFnoBlock
    up_conv: eqx.nn.ConvTranspose1d
    merge_conv: eqx.nn.Conv1d # Added for channel reduction after concatenation
    decoder_fno_block: FiLMedFnoBlock 
    out_proj: eqx.nn.Conv1d
    activation: Callable

    def __init__(self_obj, in_channels: int, out_channels: int, hidden_channels: int, embedding_dim: int, modes: int, activation: Callable, *, key):
        keys = jr.split(key, 8) 
        self_obj.activation = activation
        
        # Initial projection: Map input features (u + derivatives) to base hidden channels
        self_obj.in_proj = eqx.nn.Conv1d(in_channels, hidden_channels, kernel_size=1, key=keys[0])

        # Encoder path: FNO block at full resolution
        self_obj.encoder_fno_block = FiLMedFnoBlock(hidden_channels, embedding_dim, modes, activation, key=keys[1])

        # Downsampling convolution: Reduce spatial dimension by 2, increase channels by 2
        # (hidden_channels, N) -> (2*hidden_channels, N/2)
        self_obj.down_conv = eqx.nn.Conv1d(hidden_channels, 2 * hidden_channels, kernel_size=3, stride=2, padding=1, key=keys[2])

        # Bottleneck FNO block: Operate at half resolution with doubled channels
        self_obj.bottleneck_fno_block = FiLMedFnoBlock(2 * hidden_channels, embedding_dim, modes, activation, key=keys[3])

        # Upsampling convolution: Restore spatial dimension, reduce channels back to base hidden_channels
        # (2*hidden_channels, N/2) -> (hidden_channels, N)
        self_obj.up_conv = eqx.nn.ConvTranspose1d(2 * hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1, key=keys[4])

        # Merge convolution: Reduces channels after skip concatenation.
        # (2*hidden_channels, N) -> (hidden_channels, N)
        self_obj.merge_conv = eqx.nn.Conv1d(2 * hidden_channels, hidden_channels, kernel_size=1, key=keys[5])

        # Decoder path FNO block: Processes the upsampled features + skip connection
        self_obj.decoder_fno_block = FiLMedFnoBlock(hidden_channels, embedding_dim, modes, activation, key=keys[6])
        
        # Final projection: Map base hidden channels to output channels (1 for residual)
        self_obj.out_proj = eqx.nn.Conv1d(hidden_channels, out_channels, kernel_size=1, key=keys[7]) 

    def __call__(self_obj, x: jnp.ndarray, embedding: jnp.ndarray) -> jnp.ndarray:
        # x: (in_channels, spatial_dims)

        # Initial projection
        x_features = self_obj.in_proj(x) # (hidden_channels, N)

        # Encoder path
        x_enc = self_obj.encoder_fno_block(x_features, embedding) # (hidden_channels, N) - this is the skip connection source

        # Downsample
        x_down = self_obj.activation(self_obj.down_conv(x_enc)) # (2*hidden_channels, N/2)

        # Bottleneck
        x_bottleneck = self_obj.bottleneck_fno_block(x_down, embedding) # (2*hidden_channels, N/2)

        # Upsample
        x_up = self_obj.activation(self_obj.up_conv(x_bottleneck)) # (hidden_channels, N)
        
        # Handle potential size mismatch after ConvTranspose1d (e.g. if original N was odd)
        if x_up.shape[-1] != x_features.shape[-1]:
            x_up = x_up[..., :x_features.shape[-1]]

        # Skip connection: concatenate encoder features (x_enc) with upsampled features (x_up)
        x_concat = jnp.concatenate([x_up, x_enc], axis=0) # (2*hidden_channels, N)

        # Merge channels
        x_merged = self_obj.activation(self_obj.merge_conv(x_concat)) # (hidden_channels, N)

        # Decoder FNO block
        x_dec = self_obj.decoder_fno_block(x_merged, embedding) # (hidden_channels, N)

        # Final projection
        return self_obj.out_proj(x_dec)

# The top-level model, now using the FiLM-FNO architecture with derivative and u^2 inputs
class EquationAwareModel(eqx.Module):
    encoder: EquationEncoder
    network: FiLMedFnoUNet # Changed to UNet
    encoding_mean: jnp.ndarray # Added for normalization
    encoding_std: jnp.ndarray  # Added for normalization

    def __init__(self_obj, encoder: EquationEncoder, network: FiLMedFnoUNet, encoding_mean, encoding_std): # Renamed self to self_obj to avoid collision
        self_obj.encoder = encoder
        self_obj.network = network
        self_obj.encoding_mean = encoding_mean
        self_obj.encoding_std = encoding_std

    def __call__(self_obj, u, encoding_vector): # Renamed self to self_obj to avoid collision
        # Normalize the encoding vector before passing it to the encoder
        normalized_encoding_vector = (encoding_vector - self_obj.encoding_mean) / self_obj.encoding_std
        embedding = self_obj.encoder(normalized_encoding_vector)
        
        # Compute u^2, u*u_x, and derivatives, then concatenate with original u
        # u: (1, N)
        # compute_periodic_derivatives(u): (6, N) for (u_squared, u_u_x, u_x, u_xx, u_xxx, u_xxxx)
        u_and_derived_features = jnp.concatenate([u, compute_periodic_derivatives(u)], axis=0) # Total 1 + 6 = 7 channels
        
        # Predict the residual using the network with expanded input
        residual = self_obj.network(u_and_derived_features, embedding)
        return u + residual

# This is our custom model definition function
def PI_FNO_UNET(
    num_spatial_dims: int,
    in_channels: int, # Original in_channels (1 for u)
    encoding_dim: int,
    key,
    encoding_mean: jnp.ndarray, # Added for normalization
    encoding_std: jnp.ndarray,  # Added for normalization
):
    """Defines a generalized model that takes state `u` and an equation encoding vector."""
    embedding_dim = 32
    encoder_hidden = 64
    cnn_hidden = 128 
    num_fourier_modes = 32 
    activation_fn = jnn.silu

    # The actual input channels to FiLMedFnoUNet will be 1 (for u) + 1 (for u^2) + 1 (for u*u_x) + 4 (for derivatives) = 7
    network_input_channels = in_channels + 1 + 1 + 4 # 1 for u, 1 for u^2, 1 for u*u_x, 4 for derivatives
    # The output channels of FiLMedFnoUNet should be 1, to match `u` for residual connection
    network_output_channels = in_channels # Which is 1

    print(f"\n>>> Instantiating GENERALIZED FiLM-FNO U-Net model with DERIVATIVES, U^2, and U*UX as input: "
          f"embedding_dim={embedding_dim}, hidden_channels={cnn_hidden}, modes={num_fourier_modes}, "
          f"input_channels_to_fnonet={network_input_channels}, output_channels_from_fnonet={network_output_channels}, activation=SiLU <<<")

    encoder_key, network_key = jr.split(key)

    # 1. Define the equation encoder
    encoder = EquationEncoder(
        in_dim=encoding_dim,
        out_dim=embedding_dim,
        hidden_dim=encoder_hidden,
        activation=activation_fn,
        key=encoder_key,
    )

    # 2. Define the FiLMed FNO UNet
    network = FiLMedFnoUNet( 
        in_channels=network_input_channels, 
        out_channels=network_output_channels, 
        hidden_channels=cnn_hidden,
        embedding_dim=embedding_dim,
        modes=num_fourier_modes,
        activation=activation_fn,
        key=network_key,
    )

    # 3. Combine into the final model
    generalized_model = EquationAwareModel(encoder, network, encoding_mean, encoding_std)
    return generalized_model
